{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cтохастический градиентный спуск в задачах регрессии\n",
    "\n",
    "Отличается от простой линейно модели тем, что применяет для подстройки весов [Алгоритм Градиентного Спуска](https://ru.wikipedia.org/wiki/%D0%93%D1%80%D0%B0%D0%B4%D0%B8%D0%B5%D0%BD%D1%82%D0%BD%D1%8B%D0%B9_%D1%81%D0%BF%D1%83%D1%81%D0%BA).\n",
    "\n",
    "\n",
    "![Градиентный спуск](https://upload.wikimedia.org/wikipedia/commons/7/79/Gradient_descent.png \"Градиентный спуск на плоскости\")|\n",
    "\n",
    "![Градиентный спуск](https://upload.wikimedia.org/wikipedia/commons/6/68/Gradient_ascent_%28surface%29.png \"Градиентный спуск в пространстве\")\n",
    "\n",
    "![Градиентный спуск](https://upload.wikimedia.org/wikipedia/commons/f/f3/Stogra.png \"Поведение функции потерь градиентного спуска (ошибки модели)\")\n",
    "\n",
    "Алгоритм заключается в итеративном подстраивании весов модели с использованием входного и выходного наборов данных: на каждом шаге алгоритм вычисляет предсказание модели на одном примере из обучающей выборки, после чего вычисляет *ошибку*, используя выходной набор данных (реальные значения). Из этой ошибки формируется **антиградиент**\n",
    "\n",
    "Формула (матричный вид):\n",
    "\n",
    "$\\huge W_t=W_{t-1}+\\eta \\nabla Q(W)$,\n",
    "\n",
    "где:\n",
    "\n",
    "* $\\large -\\nabla Q(W)=\\{\\frac{\\partial Q}{\\partial w_1}, \\frac{\\partial Q}{\\partial w_2}, ..., \\frac{\\partial Q}{\\partial w_n} \\}$ - **антиградиент** ошибки относительно весов (параметров) W;\n",
    "* $\\large W_t$ - новая матрица (вектор) весов;\n",
    "* $\\large W_{t-1}$ - старая матрица (вектор) весов;\n",
    "* $\\large \\eta$ - скорость обучения;\n",
    "* $\\large n$ - количество весов (параметров) в W.\n",
    "\n",
    "Формула для $Q$ определяется выбранным функционалом ошибки (критерием потерь). Например, для MSE критерий ошибки будет:\n",
    "\n",
    "$\\huge Q=\\frac{1}{n}\\sum_{i=1}^{n} (y_{i}-g_{i})^2$,\n",
    "\n",
    "где:\n",
    "\n",
    "$\\large n$ - количество значений (семплов);\n",
    "\n",
    "$\\large y_{i}$ - предсказание из i-го индекса в наборе всех значений;   \n",
    "\n",
    "$\\large g_{i}$ - реальное значение из i-го индекса в наборе всех значений.\n",
    "\n",
    "Отсюда, принимая во внимание, что $y_{i}=f(x_{i})$ - формула модели, каждая из компонет $\\nabla Q(W)$, будет:\n",
    "\n",
    "$\\huge \\frac{\\partial Q}{\\partial w_i}=(f'(x_{i}))(f(x_{i})-g_{i})$\n",
    "\n",
    "или в матричном виде:\n",
    "\n",
    "$\\huge \\nabla Q=(f'(x))(f(x)-g)=\\frac{2}{n}(f'(x))\\times E(x)$\n",
    "\n",
    "Из зтого следует, что основной проблемой будет нахождение производной функции модели $f(x)$. Если она заранее известна, то все сводится к ее вычислению по готовой формуле. Если вы заранее *не знаете*, как она выглядит, то вам будет необходимо ([Решение этой проблемы подробно освещено в дискуссии на stackoverflow](https://stackoverflow.com/questions/9876290/how-do-i-compute-derivative-using-numpy)):\n",
    "\n",
    "1. Использовать конечное дифференцирование:\n",
    "    Допустим, функция `fun` - это наша исходная функция модели; Тогда в следующем коде:\n",
    "    ```python\n",
    "       def fun(x):\n",
    "           return x**2 + 1\n",
    "       \n",
    "       def d_fun(x):\n",
    "           h = 1e-5\n",
    "           return (fun(x+h)-fun(x-h))/(2*h)\n",
    "    ```\n",
    "    функция `d_fun` - это конечная производная (аппроксимация) с небольшим приращением `h`.\n",
    "    \n",
    "2. Использовать \"хитрую\" функцию дифференцирования из стороннего пакета (numpy по умолчанию такой функции не содержит):\n",
    "    Например, пакет \"autograd\" позволяет дифференцировать произвольные функции. Установите его командой в консоли:\n",
    "    ```bash\n",
    "    pip install autograd\n",
    "    ```\n",
    "    После, используйте ее следующим образом (обратите внимание на `grad`):\n",
    "    ```python\n",
    "       import numpy as np\n",
    "       from autograd import grad\n",
    "        \n",
    "       def fct(x):\n",
    "           y = x**2+1\n",
    "           return y\n",
    "        \n",
    "       grad_fct = grad(fct)\n",
    "       print(grad_fct(1.0))\n",
    "    ```\n",
    "    \n",
    "Достоинства:\n",
    "- Много параметров для настройки (можно оптимизировать модель);\n",
    "- Хорош для большого кол-ва данных (>10,000).\n",
    "\n",
    "Недостатки:\n",
    "- чувситвителен к размерности входных данных: необходимо *нормировать* данные.\n",
    "- множество неявных побочных эффектов (переобучение, паралич, и т.д.)\n",
    "\n",
    "[Википедия](https://en.wikipedia.org/wiki/Stochastic_gradient_descent)\n",
    "\n",
    "[Доументация (Обучающийся регрессор)](https://scikit-learn.org/stable/modules/generated/sklearn.linear_model.SGDRegressor.html#sklearn.linear_model.SGDRegressor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SGDRegressor(alpha=0.0001, average=False, epsilon=0.1, eta0=0.01,\n",
      "       fit_intercept=True, l1_ratio=0.15, learning_rate='invscaling',\n",
      "       loss='squared_loss', max_iter=1000, n_iter=None, penalty='l2',\n",
      "       power_t=0.25, random_state=None, shuffle=True, tol=0.001, verbose=0,\n",
      "       warm_start=False)\n"
     ]
    }
   ],
   "source": [
    "# Создание простейшей обучаемой модели в sklearn\n",
    "import numpy as np\n",
    "from sklearn import linear_model\n",
    "n_samples, n_features = 10, 5\n",
    "np.random.seed(0)\n",
    "y = np.random.randn(n_samples)\n",
    "X = np.random.randn(n_samples, n_features)\n",
    "clf = linear_model.SGDRegressor(max_iter=1000, tol=1e-3)\n",
    "print(clf.fit(X, y))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.59113601,  0.52024237,  1.31387889,  1.39609664,  0.52466969,\n",
       "        0.1215026 ,  1.05637942, -0.13433001,  0.73800374,  0.15920071])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# предсказание\n",
    "clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# изменение размерности данных - необходимо перед обучением модели\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "scaler = StandardScaler()\n",
    "scaler.fit(X)  # Don't cheat - fit only on training data\n",
    "X_train = scaler.transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function unary_to_nary.<locals>.nary_operator.<locals>.nary_f at 0x7f37a5d3be18>\n",
      "2.0\n"
     ]
    }
   ],
   "source": [
    "# конечное дифференцирование\n",
    "import numpy as np\n",
    "from autograd import grad\n",
    "\n",
    "def fct(x):\n",
    "    y = x**2+1\n",
    "    return y\n",
    "\n",
    "grad_fct = grad(fct)\n",
    "print(grad_fct)\n",
    "print(grad_fct(1.0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
